{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "CNN-word2vec-RU_main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/9jam/w266-final-project/blob/main/CNN_word2vec_RU_main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRAeF8g6SnPK"
      },
      "source": [
        "## BASELINE: CNN with WORD2VEC embeddings\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kemi0aPxSnPM"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "from nltk.data import find\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import gensim\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import seaborn as sns\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, Input, Dense, Lambda\n",
        "from tensorflow.keras.models import Model\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqAJ1-lmSnPN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ab30b95-aac4-4dd6-beb4-7b61c4de6e30"
      },
      "source": [
        "!pip install gensim"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.1.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPqq2iBDSnPV"
      },
      "source": [
        "### 1. LOAD the dataset\n",
        "\n",
        "Synthetically perturbed positive examples and negative preserved examples "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XP7J_6RzTPjc",
        "outputId": "3cd23ef3-32d5-4ea1-9aad-4691b57b1786"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "3ywDhleoTQvC",
        "outputId": "69019290-9dae-43c6-b4a9-38b5e860f290"
      },
      "source": [
        "meduza_bal = pd.read_pickle(\"/content/drive/MyDrive/meduza_bert_big_df.pkl\") #(\"/content/drive/MyDrive/meduza_bert_30к_df.pkl\")\n",
        "meduza_bal"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "      <th>orig_word</th>\n",
              "      <th>new_word</th>\n",
              "      <th>case</th>\n",
              "      <th>shift_case</th>\n",
              "      <th>adj</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>58048</th>\n",
              "      <td>Летом 2015 года губернатор Севастополя Сергей ...</td>\n",
              "      <td>0</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64189</th>\n",
              "      <td>« Motherla d Wi dows » « Я ухожу.bmp » « Time ...</td>\n",
              "      <td>0</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141375</th>\n",
              "      <td>Исход боя казался решенным.</td>\n",
              "      <td>0</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113185</th>\n",
              "      <td>— Раньше не было, раньше все уступали друг дру...</td>\n",
              "      <td>1</td>\n",
              "      <td>другу</td>\n",
              "      <td>другом</td>\n",
              "      <td>Dat</td>\n",
              "      <td>Ins</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>252003</th>\n",
              "      <td>В суде сообщили, что Искаков обвиняется в напа...</td>\n",
              "      <td>1</td>\n",
              "      <td>нападении</td>\n",
              "      <td>нападению</td>\n",
              "      <td>Loc</td>\n",
              "      <td>Dat</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110268</th>\n",
              "      <td>12 июня в Москве прошел марш в поддержке корре...</td>\n",
              "      <td>1</td>\n",
              "      <td>поддержку</td>\n",
              "      <td>поддержке</td>\n",
              "      <td>Acc</td>\n",
              "      <td>Dat</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>259178</th>\n",
              "      <td>Инцидент произошел с истребитель-бомбардировщи...</td>\n",
              "      <td>1</td>\n",
              "      <td>истребителем-бомбардировщиком</td>\n",
              "      <td>истребитель-бомбардировщик</td>\n",
              "      <td>Ins</td>\n",
              "      <td>Acc</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90686</th>\n",
              "      <td>— Я считаю, что это был бы лучший флешмоб, кот...</td>\n",
              "      <td>0</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131932</th>\n",
              "      <td>« музей закрыт на неопределенное время.</td>\n",
              "      <td>1</td>\n",
              "      <td>Музей</td>\n",
              "      <td>музей</td>\n",
              "      <td>Nom</td>\n",
              "      <td>Acc</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121958</th>\n",
              "      <td>Начальником за это ничего не было, его даже по...</td>\n",
              "      <td>1</td>\n",
              "      <td>Начальнику</td>\n",
              "      <td>Начальником</td>\n",
              "      <td>Dat</td>\n",
              "      <td>Ins</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>602991 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     text  ...   adj\n",
              "58048   Летом 2015 года губернатор Севастополя Сергей ...  ...  None\n",
              "64189   « Motherla d Wi dows » « Я ухожу.bmp » « Time ...  ...  None\n",
              "141375                        Исход боя казался решенным.  ...  None\n",
              "113185  — Раньше не было, раньше все уступали друг дру...  ...     0\n",
              "252003  В суде сообщили, что Искаков обвиняется в напа...  ...     0\n",
              "...                                                   ...  ...   ...\n",
              "110268  12 июня в Москве прошел марш в поддержке корре...  ...     0\n",
              "259178  Инцидент произошел с истребитель-бомбардировщи...  ...     0\n",
              "90686   — Я считаю, что это был бы лучший флешмоб, кот...  ...  None\n",
              "131932            « музей закрыт на неопределенное время.  ...     0\n",
              "121958  Начальником за это ничего не было, его даже по...  ...     0\n",
              "\n",
              "[602991 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilrHPSZPWoun"
      },
      "source": [
        "raw_data = np.array((meduza_bal.text)) \n",
        "labels = np.array((meduza_bal.target)) "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXgaRqVaWqlc"
      },
      "source": [
        "import re\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower().replace(\"ё\", \"е\")\n",
        "    #text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL', text)\n",
        "    #text = re.sub('@[^\\s]+','USER', text)\n",
        "    text = re.sub('[1-9]+', 'цфр', text)\n",
        "    text = re.sub('[^a-zA-Zа-яА-Я1-9]+', ' ', text)\n",
        "    #text = re.sub(' +',' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "data = [preprocess_text(t) for t in raw_data]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ggxleb1WhkE"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=2)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk5f2LWC7ZIU"
      },
      "source": [
        "### 2. LOAD the embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_xHGw8MTWGg"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "# Load the trained model\n",
        "w2v_model = Word2Vec.load('/content/drive/MyDrive/w266_project/models/word2vec/meduza_w2v_big.w2v')\n",
        "DIM = w2v_model.vector_size"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFfND1_YUyps"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "SENTENCE_LENGTH = 100\n",
        "NUM = len(w2v_model.wv.vocab)\n",
        "\n",
        "def get_sequences(tokenizer, x):\n",
        "    sequences = tokenizer.texts_to_sequences(x)\n",
        "    return pad_sequences(sequences, maxlen=SENTENCE_LENGTH)\n",
        "\n",
        "tokenizer = Tokenizer(num_words=NUM)\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "\n",
        "x_train_seq = get_sequences(tokenizer, x_train)\n",
        "x_test_seq = get_sequences(tokenizer, x_test)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHJPpNXXU0DH"
      },
      "source": [
        "# Embedding matrix initialization\n",
        "embedding_matrix = np.zeros((NUM, DIM))\n",
        "# Add NUM=XX most frequent words\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i >= NUM:\n",
        "        break\n",
        "    if word in w2v_model.wv.vocab.keys():\n",
        "        embedding_matrix[i] = w2v_model.wv[word]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIY8fom2VAvH"
      },
      "source": [
        "### 3. Build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqENGGp2SnPX"
      },
      "source": [
        "MAX_SEQUENCE_LENGTH = 100  # Keras' embedding layer expects a specific input length. Padding is often needed here.\n",
        "\n",
        "embedding_layer = Embedding(embedding_matrix.shape[0],\n",
        "                            embedding_matrix.shape[1],\n",
        "                            embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            trainable=False)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMaXev0KSnPX"
      },
      "source": [
        "try:\n",
        "    del tf_model\n",
        "except:\n",
        "    pass"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsK2Vt57SnPY"
      },
      "source": [
        "Now let's build the model (again as a **Sequential Model**). Now, we replace the concatination with a 1D CNN layer and a max-pooling operation. Let's choose 10 filters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdRPKH0hSnPY"
      },
      "source": [
        "tf_model = tf.keras.Sequential()\n",
        "\n",
        "tf_model.add(embedding_layer)                                        # embedding layer\n",
        "\n",
        "tf_model.add(tf.keras.layers.Conv1D(\n",
        "    filters=100, \n",
        "    kernel_size=5, \n",
        "    strides=1, \n",
        "    padding='same', \n",
        "    activation='relu', \n",
        "    use_bias=True,\n",
        "    kernel_initializer='glorot_uniform', \n",
        "    bias_initializer='zeros')\n",
        "            )    \n",
        "tf_model.add(tf.keras.layers.Dropout(0.5))\n",
        "tf_model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
        "\n",
        "\n",
        "tf_model.add(Dense(100, activation='relu'))                          # hidden layer\n",
        "tf_model.add(Dense(1, activation='sigmoid'))                         # classification layer"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MSf2twzSnPZ"
      },
      "source": [
        "Let's look at dimensions and parameters of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcAYfSgASnPZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "324f10cd-2aa4-4ce0-ec8e-38d242f2db5e"
      },
      "source": [
        "tf_model.summary()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 100, 200)          22151400  \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 100, 100)          100100    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 100, 100)          0         \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_1 (Glob (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 22,261,701\n",
            "Trainable params: 110,301\n",
            "Non-trainable params: 22,151,400\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIF2aGyRSnPZ"
      },
      "source": [
        "Like last week... let's compile the model. I.e, define optimizer, loss function, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gvhas8O5SnPZ"
      },
      "source": [
        "tf_model.compile(optimizer='adam', loss='binary_crossentropy', metrics='accuracy')"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpZEFL1TSnPd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3202853-4ba6-4db0-a59b-e42e8bbf94d5"
      },
      "source": [
        "tf_model.fit(x_train_seq, y_train, validation_data=(x_test_seq, y_test), epochs=1)\n",
        "tf_model.fit(x_train_seq, y_train, validation_data=(x_test_seq, y_test), epochs=150, verbose=0)\n",
        "tf_model.fit(x_train_seq, y_train, validation_data=(x_test_seq, y_test), epochs=1)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15075/15075 [==============================] - 59s 4ms/step - loss: 0.6793 - accuracy: 0.5731 - val_loss: 0.6737 - val_accuracy: 0.5880\n",
            "15075/15075 [==============================] - 59s 4ms/step - loss: 0.5747 - accuracy: 0.6899 - val_loss: 0.6079 - val_accuracy: 0.6719\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f531457a890>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2R0wDDnSnPd"
      },
      "source": [
        "Look's good! Actually better than last week... but don't make much of that, given this crazy simple data set. \n",
        "\n",
        "What are train & test predictions now?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Okn84PdXSnPe"
      },
      "source": [
        "y_pred = tf_model.predict(x_test_seq)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUi_N99rgPOV"
      },
      "source": [
        "y_pred_bin = [int(x > 0.5) for x in y_pred]"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZiLaSU1fiAo",
        "outputId": "3d9004b2-c2bd-4bb8-ac81-e39906deee47"
      },
      "source": [
        "print(classification_report(y_test,y_pred_bin))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.81      0.73     65482\n",
            "           1       0.70      0.50      0.58     55117\n",
            "\n",
            "    accuracy                           0.67    120599\n",
            "   macro avg       0.68      0.66      0.66    120599\n",
            "weighted avg       0.68      0.67      0.66    120599\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIfUE38WSnPe"
      },
      "source": [
        "Yey! But we obviously cheated here with the choice of sentences. Nevertheless, the idea should be clear."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaftwAXHSnPe"
      },
      "source": [
        "**Questions for the class for joint live in-class exercises**:\n",
        "\n",
        "1) Can you relate the value for the validation loss to the prediction for the test set \n",
        "\n",
        "2) What do you think happens if you change the 'trainable' flag in the embedding layer from 'False' to 'True'?   \n",
        "\n",
        "3) What do you need to change in the model if you want more filters of the same kernel size?    \n",
        "\n",
        "**Note/Question:** What would you need to change if you wanted to add CNN layers (at the same position) of different kernel sizes? That gets us to Keras Functional API... "
      ]
    }
  ]
}
